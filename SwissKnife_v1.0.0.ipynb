{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ceec1a3f-6cc0-45cf-9360-8f83c92bfafc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Guidelines\n",
    "\n",
    "**Note:** This notebook aims to make our life easier and provide quick and nice workable solutions. Feel free to suggest changes or performance improvements. All cells must be \"self executable\" in order to isolate the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee44121-0894-4b3d-9786-da7dd81f334f",
     "showTitle": true,
     "title": "Removing duplicates with window instead distinct/drop duplicates"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">\n",
       "\n",
       "GETTING THE DATA WITHOUT JOINING WITH ITSELF OR OTHER POTENTIAL LOOKUP TABLE\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">\n\nGETTING THE DATA WITHOUT JOINING WITH ITSELF OR OTHER POTENTIAL LOOKUP TABLE\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>NodeId</th><th>ParentId</th></tr></thead><tbody><tr><td>A535ersgdf1</td><td></td></tr><tr><td>Bgdfgdgdsh</td><td>A535ersgdf1</td></tr><tr><td>C56464dfgdf</td><td>A535ersgdf1</td></tr><tr><td>D54654654df</td><td>Bgdfgdgdsh</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A535ersgdf1",
         ""
        ],
        [
         "Bgdfgdgdsh",
         "A535ersgdf1"
        ],
        [
         "C56464dfgdf",
         "A535ersgdf1"
        ],
        [
         "D54654654df",
         "Bgdfgdgdsh"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "NodeId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ParentId",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>NodeId</th><th>ParentId</th><th>rank</th></tr></thead><tbody><tr><td>A535ersgdf1</td><td></td><td>1</td></tr><tr><td>Bgdfgdgdsh</td><td>A535ersgdf1</td><td>1</td></tr><tr><td>C56464dfgdf</td><td>A535ersgdf1</td><td>2</td></tr><tr><td>D54654654df</td><td>Bgdfgdgdsh</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A535ersgdf1",
         "",
         1
        ],
        [
         "Bgdfgdgdsh",
         "A535ersgdf1",
         1
        ],
        [
         "C56464dfgdf",
         "A535ersgdf1",
         2
        ],
        [
         "D54654654df",
         "Bgdfgdgdsh",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "NodeId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ParentId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rank",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>NodeId</th><th>ParentId</th><th>rank</th></tr></thead><tbody><tr><td>A535ersgdf1</td><td></td><td>1</td></tr><tr><td>Bgdfgdgdsh</td><td>A535ersgdf1</td><td>1</td></tr><tr><td>D54654654df</td><td>Bgdfgdgdsh</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A535ersgdf1",
         "",
         1
        ],
        [
         "Bgdfgdgdsh",
         "A535ersgdf1",
         1
        ],
        [
         "D54654654df",
         "Bgdfgdgdsh",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "NodeId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ParentId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rank",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">\n",
       "\n",
       "DEALING WITH DUPLICATIONS!\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">\n\nDEALING WITH DUPLICATIONS!\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>NodeId</th><th>ParentId</th></tr></thead><tbody><tr><td>A535ersgdf1</td><td></td></tr><tr><td>Bgdfgdgdsh</td><td>A535ersgdf1</td></tr><tr><td>Bgdfgdgdsh</td><td>A535ersgdf1</td></tr><tr><td>D54654654df</td><td>Bgdfgdgdsh</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A535ersgdf1",
         ""
        ],
        [
         "Bgdfgdgdsh",
         "A535ersgdf1"
        ],
        [
         "Bgdfgdgdsh",
         "A535ersgdf1"
        ],
        [
         "D54654654df",
         "Bgdfgdgdsh"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "NodeId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ParentId",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>NodeId</th><th>ParentId</th><th>rank</th><th>rank2</th></tr></thead><tbody><tr><td>A535ersgdf1</td><td></td><td>1</td><td>1</td></tr><tr><td>Bgdfgdgdsh</td><td>A535ersgdf1</td><td>2</td><td>1</td></tr><tr><td>D54654654df</td><td>Bgdfgdgdsh</td><td>3</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A535ersgdf1",
         "",
         1,
         1
        ],
        [
         "Bgdfgdgdsh",
         "A535ersgdf1",
         2,
         1
        ],
        [
         "D54654654df",
         "Bgdfgdgdsh",
         3,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "NodeId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ParentId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rank",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "rank2",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, dense_rank, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\"\"\"\n",
    "This optimization technique aims to use Window to create an index and filter the rows you need instead performing a join to get the data needed. This also may be helpful when you need to remove duplicates, instead of using distinct/drop duplicates you can use the Window to generate an index and pick up the element you want.azure\n",
    "\n",
    "Why: Window triggers a narrow transformation, whereas distinct / join triggers a wide trasnfromation and consequently a shuffle in the cluster.\n",
    "\n",
    "If you are new to Narrow x Wide transformations and Spark check this quick explanation:\n",
    "https://www.linkedin.com/posts/minalbhatkar_bigdata-dataengineer-dataanalytics-activity-7072807039709700096-JRP6/\n",
    "\n",
    "\"\"\"\n",
    "print(\"\\n\")\n",
    "print(\"GETTING THE DATA WITHOUT JOINING WITH ITSELF OR OTHER POTENTIAL LOOKUP TABLE\")\n",
    "window = Window.partitionBy(\"ParentId\").orderBy(col(\"NodeId\"))\n",
    "\n",
    "sample_data = [(\"A535ersgdf1\", \"\"),\n",
    "               (\"Bgdfgdgdsh\",\"A535ersgdf1\"),\n",
    "               (\"C56464dfgdf\",\"A535ersgdf1\"),\n",
    "               (\"D54654654df\",\"Bgdfgdgdsh\")\n",
    "              ]\n",
    "\n",
    "sample_df = spark.createDataFrame(sample_data, [\"NodeId\",\"ParentId\"])\n",
    "sample_df.display()\n",
    "\n",
    "sample_ranked_df = sample_df.withColumn(\"rank\", dense_rank().over(window))\n",
    "sample_ranked_df.display()\n",
    "\n",
    "sample_ranked_df.filter(\"rank == 1\").display()\n",
    "\n",
    "###############################################################################\n",
    "print(\"\\n\")\n",
    "print(\"DEALING WITH DUPLICATIONS!\")\n",
    "\n",
    "dup_data =  [  (\"A535ersgdf1\", \"\"),\n",
    "               (\"Bgdfgdgdsh\",\"A535ersgdf1\"),\n",
    "               (\"Bgdfgdgdsh\",\"A535ersgdf1\"),\n",
    "               (\"D54654654df\",\"Bgdfgdgdsh\")\n",
    "            ]\n",
    "\n",
    "dup_df = spark.createDataFrame(dup_data, [\"NodeId\",\"ParentId\"])\n",
    "dup_df.display()\n",
    "\n",
    "window_dup = Window.orderBy(col(\"NodeId\"))\n",
    "window_dup2 = Window.partitionBy(\"NodeId\").orderBy(col(\"rank\"))\n",
    "\n",
    "(dup_df.withColumn(\"rank\", dense_rank().over(window_dup))\n",
    "       .withColumn(\"rank2\", row_number().over(window_dup2))\n",
    "       .filter(\"rank2 == 1\")\n",
    " ).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa6a3e44-e5a2-4f21-8c00-872e1a74d97e",
     "showTitle": true,
     "title": "Explode JSON (MapType) columns"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, map_entries\n",
    "\n",
    "# Note: Be careful, explode is a costly function\n",
    "\n",
    "df_explode_test = spark.read.format(\"delta\").load(\"SOME DELTA LAKE PATH\")\n",
    "\n",
    "df_with_array = df_explode_test.withColumn(\"my_map_col_array\", map_entries(F.col(\"productJSON\")))\n",
    "#df_with_array.display()\n",
    "\n",
    "exploded_df = df_with_array.withColumn(\"exploded_map\", explode(F.col(\"my_map_col_array\")))\n",
    "#exploded_df.display()\n",
    "\n",
    "\n",
    "exploded_df_final = (exploded_df.withColumn(\"key\", F.col(\"exploded_map\").getItem(\"key\"))\n",
    "                                .withColumn(\"value\", F.col(\"exploded_map\").getItem(\"value\"))\n",
    "                                .drop(\"exploded_map\", \"my_map_col_array\")\n",
    "                    )\n",
    "\n",
    "exploded_df_final.display()   \n",
    "\n",
    "# Other Example with in-memory data\n",
    "data = [(1, {\"a\": 10, \"b\": 20}),\n",
    "        (2, {\"c\": 30, \"d\": 40}),\n",
    "        (3, {\"e\": 50, \"f\": 60})]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"data\"])\n",
    "\n",
    "df.display()\n",
    "\n",
    "# Use map_entries to convert the \"data\" map column to an array of structs\n",
    "df_with_entries = df.withColumn(\"entries\", F.map_entries(col(\"data\")))\n",
    "df_with_entries.display()\n",
    "\n",
    "exploded_df = df_with_entries.withColumn(\"exploded_map\", F.explode(F.col(\"entries\")))\n",
    "exploded_df.display()\n",
    "\n",
    "exploded_df_final = (exploded_df.withColumn(\"key\", F.col(\"exploded_map\").getItem(\"key\"))\n",
    "                                .withColumn(\"value\", F.col(\"exploded_map\").getItem(\"value\"))\n",
    "                                .drop(\"exploded_map\", \"entries\")\n",
    "                    )\n",
    "exploded_df_final.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e92f7d2b-676c-41bc-98be-374c922054c2",
     "showTitle": true,
     "title": "Update values in MapType (dictionary) columns"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>data</th></tr></thead><tbody><tr><td>1</td><td>Map(a -> 10, b -> 20)</td></tr><tr><td>2</td><td>Map(d -> 40, c -> 30)</td></tr><tr><td>3</td><td>Map(e -> 50, f -> 60)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         {
          "a": 10,
          "b": 20
         }
        ],
        [
         2,
         {
          "c": 30,
          "d": 40
         }
        ],
        [
         3,
         {
          "e": 50,
          "f": 60
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "data",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"long\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>data</th><th>values</th><th>keys</th><th>values_transformed</th><th>data_map_transformed</th></tr></thead><tbody><tr><td>1</td><td>Map(a -> 10, b -> 20)</td><td>List(10, 20)</td><td>List(a, b)</td><td>List(1.0, 2.0)</td><td>Map(a -> 1.0, b -> 2.0)</td></tr><tr><td>2</td><td>Map(d -> 40, c -> 30)</td><td>List(40, 30)</td><td>List(d, c)</td><td>List(4.0, 3.0)</td><td>Map(d -> 4.0, c -> 3.0)</td></tr><tr><td>3</td><td>Map(e -> 50, f -> 60)</td><td>List(50, 60)</td><td>List(e, f)</td><td>List(5.0, 6.0)</td><td>Map(e -> 5.0, f -> 6.0)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         {
          "a": 10,
          "b": 20
         },
         [
          10,
          20
         ],
         [
          "a",
          "b"
         ],
         [
          1.0,
          2.0
         ],
         {
          "a": 1.0,
          "b": 2.0
         }
        ],
        [
         2,
         {
          "c": 30,
          "d": 40
         },
         [
          40,
          30
         ],
         [
          "d",
          "c"
         ],
         [
          4.0,
          3.0
         ],
         {
          "c": 3.0,
          "d": 4.0
         }
        ],
        [
         3,
         {
          "e": 50,
          "f": 60
         },
         [
          50,
          60
         ],
         [
          "e",
          "f"
         ],
         [
          5.0,
          6.0
         ],
         {
          "e": 5.0,
          "f": 6.0
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "data",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"long\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "values",
         "type": "{\"type\":\"array\",\"elementType\":\"long\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "keys",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "values_transformed",
         "type": "{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "data_map_transformed",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"double\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, map_from_arrays, map_values, map_keys, col, lit\n",
    "\n",
    "# Sample data with a MapType column \"data\"\n",
    "data = [(1, {\"a\": 10, \"b\": 20}),\n",
    "        (2, {\"c\": 30, \"d\": 40}),\n",
    "        (3, {\"e\": 50, \"f\": 60})]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"data\"])\n",
    "df.display()\n",
    "\n",
    "(df\n",
    ".withColumn(\"values\", map_values(col(\"data\")))\n",
    ".withColumn(\"keys\", map_keys(col(\"data\")))\n",
    ".withColumn(\"values_transformed\", expr(\"transform(values, v -> v/10)\"))\n",
    ".withColumn(\"data_map_transformed\", map_from_arrays(col(\"keys\"), col(\"values_transformed\")))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f93d270b-ed2f-4692-8cac-d5902ce7c0ba",
     "showTitle": true,
     "title": "Transform for dataframes "
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>CourseName</th><th>Fee</th><th>Discount</th></tr></thead><tbody><tr><td>Java</td><td>4000</td><td>5</td></tr><tr><td>Python</td><td>4600</td><td>10</td></tr><tr><td>Scala</td><td>4100</td><td>15</td></tr><tr><td>Scala</td><td>4500</td><td>15</td></tr><tr><td>PHP</td><td>3000</td><td>20</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Java",
         4000,
         5
        ],
        [
         "Python",
         4600,
         10
        ],
        [
         "Scala",
         4100,
         15
        ],
        [
         "Scala",
         4500,
         15
        ],
        [
         "PHP",
         3000,
         20
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "CourseName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Fee",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Discount",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>CourseName</th><th>Fee</th><th>Discount</th><th>new_fee</th><th>discounted_fee</th></tr></thead><tbody><tr><td>JAVA</td><td>4000</td><td>5</td><td>3000</td><td>2850.0</td></tr><tr><td>PYTHON</td><td>4600</td><td>10</td><td>3600</td><td>3240.0</td></tr><tr><td>SCALA</td><td>4100</td><td>15</td><td>3100</td><td>2635.0</td></tr><tr><td>SCALA</td><td>4500</td><td>15</td><td>3500</td><td>2975.0</td></tr><tr><td>PHP</td><td>3000</td><td>20</td><td>2000</td><td>1600.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "JAVA",
         4000,
         5,
         3000,
         2850.0
        ],
        [
         "PYTHON",
         4600,
         10,
         3600,
         3240.0
        ],
        [
         "SCALA",
         4100,
         15,
         3100,
         2635.0
        ],
        [
         "SCALA",
         4500,
         15,
         3500,
         2975.0
        ],
        [
         "PHP",
         3000,
         20,
         2000,
         1600.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "CourseName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Fee",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Discount",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "new_fee",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "discounted_fee",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "pyspark.sql.DataFrame.transform() is used to chain the custom transformations and this function returns the new DataFrame after applying the specified transformations.\n",
    "\n",
    "Arguments:\n",
    "function to be applied to the dataframe + other values to be considered by the function passed\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import upper, col, lit\n",
    "\n",
    "simpleData = ((\"Java\",4000, 5), \n",
    "              (\"Python\", 4600, 10),  \n",
    "              (\"Scala\", 4100, 15),   \n",
    "              (\"Scala\", 4500, 15),   \n",
    "              (\"PHP\", 3000, 20),  \n",
    "             )\n",
    "\n",
    "columns= [\"CourseName\", \"Fee\", \"Discount\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.display()\n",
    "\n",
    "\n",
    "\n",
    "# Transformation Pipeline\n",
    "# Custom transformation 1\n",
    "def to_upper_str_columns(df):\n",
    "    return df.withColumn(\"CourseName\", upper(col(\"CourseName\")))\n",
    "\n",
    "# Custom transformation 2\n",
    "def reduce_price(reduceBy):\n",
    "    def transformation(df):\n",
    "        return df.withColumn(\"new_fee\", col(\"fee\") - lit(reduceBy))\n",
    "    return transformation\n",
    "\n",
    "# Custom transformation 3\n",
    "def apply_discount(df):\n",
    "    return df.withColumn(\"discounted_fee\", col(\"new_fee\") - (col(\"new_fee\") * col(\"discount\")) / 100)\n",
    "\n",
    "df2 = df.transform(to_upper_str_columns)\n",
    "df2 = df2.transform(reduce_price(1000)) # Adjust the reduction amount as needed\n",
    "df2 = df2.transform(apply_discount)\n",
    "\n",
    "df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e08e65-b8a4-44b1-99d3-25820fa59c99",
     "showTitle": true,
     "title": "Implode complex types columns"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-356943907568170&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">7</span>\n",
       "<span class=\"ansi-red-fg\">    &#34;&#34;&#34;</span>\n",
       "    ^\n",
       "<span class=\"ansi-red-fg\">IndentationError</span><span class=\"ansi-red-fg\">:</span> expected an indented block\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-356943907568170&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">7</span>\n<span class=\"ansi-red-fg\">    &#34;&#34;&#34;</span>\n    ^\n<span class=\"ansi-red-fg\">IndentationError</span><span class=\"ansi-red-fg\">:</span> expected an indented block\n</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">IndentationError</span><span class=\"ansi-red-fg\">:</span> expected an indented block",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def implode_data(df_for_imploding_df):\n",
    "\"\"\"\n",
    "Receives a dataframe with exploded columns, picks a schema to be based on (line 11) and apply collect_list\n",
    "\"\"\"\n",
    "    window = Window.paritionBy(\"TableId\").orderBy(\"ComplexTypeColumn.NestedColumnToOrder\", \"ComplexTypeColumn.OtherId\") # Configure as the nature of your data\n",
    "\n",
    "    rank_window = Window.paritionBy(\"TableId\").orderBy(desc(\"ComplexTypeColumn.NestedColumnToOrder\")) # Configure as the nature of your data\n",
    "\n",
    "    assembled_df = (df_for_imploding_df.select(\"*\", struct(complextype_col_struct()).alias(\"ComplexTypeColumn\"))\n",
    "                                       .drop(*receipt_line_struct())\n",
    "                                       .withColumn(\"ComplexTypeColumn\", collect_list(\"ComplexTypeColumn\").over(window))\n",
    "                                       .withColumn(\"Rank\", row_number().over(rank_window))\n",
    "                                       .filter(\"Rank == 1\")\n",
    "                                       .drop(\"Rank\")\n",
    "\n",
    "    )\n",
    "\n",
    "    #complextype_col_struct() --> Nested Schema \n",
    "\n",
    "    return assembled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "443597a1-c847-4d5f-9c5e-73e615ef7aff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>item</th><th>precision</th><th>hash</th></tr></thead><tbody><tr><td>1</td><td>A</td><td>1.0</td><td>hsua-df</td></tr><tr><td>2</td><td>B</td><td>2.0</td><td>ahs-dkf</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "A",
         "1.0",
         "hsua-df"
        ],
        [
         2,
         "B",
         "2.0",
         "ahs-dkf"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "item",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "precision",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "hash",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>item</th><th>precision</th><th>hash</th><th>ComplexTypeColumn</th><th>ComplexTypeList</th><th>Rank</th></tr></thead><tbody><tr><td>1</td><td>A</td><td>1.0</td><td>hsua-df</td><td>List(A, 1.0, hsua-df)</td><td>List(List(A, 1.0, hsua-df))</td><td>1</td></tr><tr><td>2</td><td>B</td><td>2.0</td><td>ahs-dkf</td><td>List(B, 2.0, ahs-dkf)</td><td>List(List(B, 2.0, ahs-dkf))</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "A",
         "1.0",
         "hsua-df",
         [
          "A",
          "1.0",
          "hsua-df"
         ],
         [
          [
           "A",
           "1.0",
           "hsua-df"
          ]
         ],
         1
        ],
        [
         2,
         "B",
         "2.0",
         "ahs-dkf",
         [
          "B",
          "2.0",
          "ahs-dkf"
         ],
         [
          [
           "B",
           "2.0",
           "ahs-dkf"
          ]
         ],
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "item",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "precision",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "hash",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ComplexTypeColumn",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"item\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"precision\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"hash\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "ComplexTypeList",
         "type": "{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"item\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"precision\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"hash\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":false}"
        },
        {
         "metadata": "{}",
         "name": "Rank",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, col, struct, collect_list, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, \"A\", 1.0, \"hsua-df\"),\n",
    "    (2, \"B\", 2.0, \"ahs-dkf\")    \n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "         StructField(\"id\", IntegerType(), True),\n",
    "         StructField(\"item\", StringType(), True),\n",
    "         StructField(\"precision\", StringType(), True),\n",
    "         StructField(\"hash\", StringType(), True)\n",
    "])\n",
    "\n",
    "def complex_schema():\n",
    "    return StructType([\n",
    "         StructField(\"item\", StringType(), True),\n",
    "         StructField(\"precision\", StringType(), True),\n",
    "         StructField(\"hash\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "# Create the DataFrame\n",
    "df_for_imploding_df = spark.createDataFrame(data, schema)\n",
    "df_for_imploding_df.display()\n",
    "\n",
    "# Define the window specification\n",
    "window_spec = Window().partitionBy(\"id\")\n",
    "sublevel_rank_window = Window.orderBy(\"id\")\n",
    "\n",
    "# Perform the operations\n",
    "assembled_df = (df_for_imploding_df\n",
    "    .withColumn(\"ComplexTypeColumn\", struct(\"item\", \"precision\", \"hash\"))\n",
    "    .withColumn(\"ComplexTypeList\", collect_list(\"ComplexTypeColumn\").over(window_spec))\n",
    "    .withColumn(\"Rank\", row_number().over(sublevel_rank_window)) # --> Filter this rank if you have repetition in the \"id\"\n",
    ")\n",
    "# Show the resulting DataFrame\n",
    "assembled_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f84847fc-5597-4d3c-890c-d2cda12e8d57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[35]: [StructField(item,StringType,true),\n",
       " StructField(precision,StringType,true),\n",
       " StructField(hash,StringType,true)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[35]: [StructField(item,StringType,true),\n StructField(precision,StringType,true),\n StructField(hash,StringType,true)]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = complex_schema()\n",
    "list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3381fc1f-9794-4355-b894-8036e205be11",
     "showTitle": true,
     "title": "ThreadPoolExecutor"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Num of CPUs available 16\n",
       "3\n",
       "7\n",
       "11\n",
       "15\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Num of CPUs available 16\n3\n7\n11\n15\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "\n",
    "def summing_test(item):\n",
    "\n",
    "    return item[0] + item[1]\n",
    "\n",
    "\n",
    "numbers_to_sum = [(1,2), (3,4), (5,6), (7,8)]\n",
    "\n",
    "\n",
    "print(\"Num of CPUs available\", multiprocessing.cpu_count())\n",
    "\n",
    "no_of_products_to_run_in_parallel= 4 \n",
    "\n",
    "pool = ThreadPoolExecutor(no_of_products_to_run_in_parallel)\n",
    "\n",
    "results = pool.map(summing_test, numbers_to_sum) # does not block\n",
    "\n",
    "for res in results:\n",
    "    print(res) # print results as they become available\n",
    "\n",
    "pool.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f40fa5b-0f84-4008-8605-cd24b7ffcd73",
     "showTitle": true,
     "title": "Ticks to DateTime"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "@udf(TimestampType())\n",
    "def ticks_to_datetime(ticks):\n",
    "  try:\n",
    "    return datetime.datetime.fromtimestamp(float(ticks)/1000)\n",
    "  except Exception:\n",
    "    return ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a146844-80e8-420d-b135-e2bb8634574e",
     "showTitle": true,
     "title": "convert_string_to_json"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import MapType, StringType\n",
    "\n",
    "@udf(MapType(StringType(),StringType()))\n",
    "def convert_string_to_json(input_string):\n",
    "\n",
    "  key_value_pairs = {}\n",
    "\n",
    "  if input_string is not None:\n",
    "\n",
    "    split_string = [tuple(x.split(':')) for x in input_string.split(';') if len(tuple(x.split(':'))) == 2]\n",
    "\n",
    "    for k,v in split_string:\n",
    "        key_value_pairs[k] = v\n",
    "\n",
    "  return key_value_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d44cb53-e0bc-4c08-93c8-b57ce59e8342",
     "showTitle": true,
     "title": "Unpivot Dataframe"
    }
   },
   "outputs": [],
   "source": [
    "def unpivot(df, key_columns):\n",
    "\n",
    "  columns_to_pivot = [column for column in df.columns if column not in key_columns]\n",
    "  list_of_alias_columns_for_stack = []\n",
    "  \n",
    "  for column in columns_to_pivot:\n",
    "    list_of_alias_columns_for_stack.append(f\"'{column}',cast({column} AS STRING)\")\n",
    "  str_of_alias_columns_for_stack = \",\".join(list_of_alias_columns_for_stack)\n",
    "  \n",
    "  return (df.selectExpr(*key_columns, f\"stack({len(columns_to_pivot)}, {str_of_alias_columns_for_stack}) as (Name, Value)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b5da204-1768-420d-822b-99d25a4de59a",
     "showTitle": true,
     "title": "Write multiple dataframe into CSV"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code extracts the power of workers + driver to write CSVs besides to use a for loop to save file by file\n",
    "\n",
    "You can use the thread pool executor for an extra layer of paralellism (1 write per thread)\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Sample data\n",
    "data = [(\"apple\", 1), (\"orange\", 2), (\"banana\", 3), (\"apple\", 4), (\"apple\", 5), (\"orange\", 6)]\n",
    "df_fruits = spark.createDataFrame(data, [\"fruit\", \"id\"])\n",
    "\n",
    "# Define queries\n",
    "queries = {}\n",
    "queries[\"1\"] = \"apple\"\n",
    "queries[\"2\"] = \"orange\"\n",
    "\n",
    "def filter_and_write_dataframe(df, query, delimiter=\",\", header=True, mode=\"overwrite\"):\n",
    "    # Filter the DataFrame based on the query value\n",
    "    filtered_df = df.filter(col(\"fruit\") == query)\n",
    "    \n",
    "    # Write the filtered DataFrame to CSV\n",
    "    filtered_df.write.csv(path=\"Your-Path-Here\" + query, sep=delimiter, header=header, mode=mode)\n",
    "\n",
    "# Create separate DataFrames for each query\n",
    "dfs = {query: df_fruits.filter(col(\"fruit\") == query) for query in queries.values()}\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "for query, df in dfs.items():\n",
    "    filter_and_write_dataframe(df, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f8ae61e-d8ee-4865-aa51-a2ae43866497",
     "showTitle": true,
     "title": "Create Spark Dataframe from Dictionary"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataframe_from_dict(list_of_dict_rows, schema, enforce_none_policy=True):\n",
    "  \n",
    "  final_list_of_rows = []\n",
    "  \n",
    "  for dict_row in list_of_dict_rows:\n",
    "    row = []\n",
    "    for column in schema.names:\n",
    "      if enforce_none_policy and not schema[column].nullable:\n",
    "        assert column in dict_row, f\"The enforce_none_policy is True and the column {column} doesn't accept nulls\"\n",
    "      row.append(list_of_dict_rows.get(column, None)) # insert None if doesn't exist\n",
    "    final_list_of_rows.append(row)\n",
    "  \n",
    "  return spark.createDataFrame(final_list_of_rows, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a5f7897-1e36-4146-9605-cc2f58d081c6",
     "showTitle": true,
     "title": "Convert string of semi-colon separated key/value pairs to json"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import MapType, StringType\n",
    "\n",
    "@udf(MapType(StringType(),StringType()))\n",
    "def convert_string_to_json(input_string):\n",
    "\n",
    "  key_value_pairs = {}\n",
    "\n",
    "  if input_string is not None:\n",
    "\n",
    "    split_string = [tuple(x.split(':')) for x in input_string.split(';') if len(tuple(x.split(':'))) == 2]\n",
    "\n",
    "    for k,v in split_string:\n",
    "        key_value_pairs[k] = v\n",
    "\n",
    "  return key_value_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d042fa81-57cb-4bec-86fd-e56c4f897ee5",
     "showTitle": true,
     "title": "Convert string to json to get dictionary - UDF version for dataframes"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import MapType, StringType\n",
    "\n",
    "@udf(MapType(StringType(),StringType()))\n",
    "def convert_string_to_json_for_target_weights(input_string):\n",
    "\n",
    "  return json.loads(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f776988-fa79-4344-8846-6fa5db6ae5ec",
     "showTitle": true,
     "title": "Generic Aggregations"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_timestamp\n",
    "import pandas as pd \n",
    "\n",
    "# Check duplicates easily\n",
    "data \\\n",
    "    .groupby(['Id']) \\\n",
    "    .count() \\\n",
    "    .where('count > 1') \\\n",
    "    .sort('count', ascending= False) \\\n",
    "    .show()\n",
    "\n",
    "# Select distinct values from given column \t\n",
    "df_demog = read_demog_updates(running_date)\n",
    "display(df_demog.select(\"Name\").distinct())\n",
    "\n",
    "# Check NULL / NOT NULL values\n",
    "df.where(col(\"dt_mvmt\").isNull())\n",
    "df.where(col(\"dt_mvmt\").isNotNull())\n",
    "\n",
    "# Drop Null values\n",
    "df.na.drop(subset=[\"dt_mvmt\"])\n",
    "\n",
    "# When with isIn \n",
    "display(almost_result.withColumn(\"Lifestage\", \n",
    "                           when(col(\"NumberOfChildren\").isin(\"0\",\"1\",\"2\",\"3\",\"4\"), \"Young Family 0-4 Years\")\n",
    "                          .when(col(\"NumberOfChildren\").isin(\"5\",\"6\",\"7\",\"8\",\"9\"), \"Middle Family 5-9 Years\")\n",
    "                          .when(col(\"NumberOfChildren\").isin(\"10\",\"11\",\"12\",\"13\",\"14\",\"15\"), \"Family 10+ Years\")\n",
    "                          .otherwise(\"Unknown\")\n",
    "             ))\t\t \n",
    "\t\t\t \n",
    "# Pyspark between\n",
    "test_df.filter(col(\"start\").between(pd.to_datetime('2023-04-13'), pd.to_datetime('2023-04-14'))).show()\n",
    "\n",
    "# Import stuffs\n",
    "myTest =(spark.read.format(\"csv\")\n",
    "            .schema(myschema())\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"delimiter\", \"\\ua746\")\n",
    "            .load(\"your-path\"))\n",
    "\n",
    "# Group by columns aggregating the value and renaming the aggregation\t\t\t\n",
    "display(test__df.groupby(\"Id1\",\"Id2\", \"Name\").agg(min(col(\"Value\").cast(IntegerType()))).withColumnRenamed(\"min(CAST(Value AS INT))\", \"minAge\"))\n",
    "\n",
    "\n",
    "#### MERGE EXAMPLE!!! \n",
    "time_now = current_timestamp()\n",
    "deltaTable = DeltaTable.forPath(spark, \"your-delta-table-path\")\n",
    "\n",
    "deltaTable.alias(\"tableA\").merge(\n",
    "    updatesDF_usis.alias(\"updates\"),\n",
    "    \"tableA.Id = updates.Id\") \\\n",
    "  .whenMatchedUpdate(set = {'Id': 'updates.Id',\n",
    "                            'RecipeGroup': 'updates.RecipeGroup',\n",
    "                            'ShopId': 'updates.ShopId',\n",
    "                            'Location': 'updates.Location',\n",
    "                            'Address': 'updates.Address',\n",
    "                            'Postcode': 'updates.Postcode',\n",
    "                            'siteURL': 'updates.siteURL',\n",
    "                            'UpdatedBy': 'updates.UpdatedBy',\n",
    "                            'CreatedDateTime': 'usis.CreatedDateTime',\n",
    "                            'UpdatedDateTime': lit(time_now),\n",
    "                            'CreatedBy': 'usis.CreatedBy',\n",
    "                            }\n",
    ") \\\n",
    "  .whenNotMatchedInsert(values = {'Id': 'updates.Id',\n",
    "                            'RecipeGroup': 'updates.RecipeGroup',\n",
    "                            'ShopId': 'updates.ShopId',\n",
    "                            'Location': 'updates.Location',\n",
    "                            'Address': 'updates.Address',\n",
    "                            'Postcode': 'updates.Postcode',\n",
    "                            'siteURL': 'updates.siteURL',\n",
    "                            'UpdatedBy': 'updates.UpdatedBy',\n",
    "                            'CreatedDateTime': lit(time_now),\n",
    "                            'UpdatedDateTime': lit(time_now),\n",
    "                            'CreatedBy': 'usis.CreatedBy',\n",
    "                                 }\n",
    ") \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75bda96-0575-4129-9505-7798f90dd707",
     "showTitle": true,
     "title": "UDF Example"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>SeqNo</th><th>Name</th></tr></thead><tbody><tr><td>1</td><td>john jones</td></tr><tr><td>2</td><td>tracey smith</td></tr><tr><td>3</td><td>amy sanders</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "john jones"
        ],
        [
         "2",
         "tracey smith"
        ],
        [
         "3",
         "amy sanders"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "SeqNo",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>SeqNo</th><th>Name</th><th>NameUpperCase</th></tr></thead><tbody><tr><td>1</td><td>john jones</td><td>JOHN JONES</td></tr><tr><td>2</td><td>tracey smith</td><td>TRACEY SMITH</td></tr><tr><td>3</td><td>amy sanders</td><td>AMY SANDERS</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "john jones",
         "JOHN JONES"
        ],
        [
         "2",
         "tracey smith",
         "TRACEY SMITH"
        ],
        [
         "3",
         "amy sanders",
         "AMY SANDERS"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "SeqNo",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "NameUpperCase",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "from pyspark.sql.types import StringType \n",
    "\n",
    "\"\"\"\n",
    "UDF (a.k.a User Defined Function) is the most useful feature of Spark SQL & DataFrame that is used to extend the PySpark build in capabilities. You can use with Select, WithColumn, and SQL\n",
    "\n",
    "Note: UDF’s are the most expensive operations hence use them only you have no choice and when essential logic in spark would be overcomplicated. They are distributable but work like black box and you may face serialization problems when running with a cluster with workers\n",
    "\n",
    "Pros: You can easily expand your logic\n",
    "\n",
    "Cons: Catalyst Optimizer does not optimize the code very well. Built-in pyspark functions are more efficient and parallizable friendly\n",
    "\n",
    "\"\"\"\n",
    "# UDFs return string by default\n",
    "@udf(returnType=StringType()) \n",
    "def upperCase(myString):\n",
    "    return myString.upper()\n",
    "\n",
    "\n",
    "columns = [\"SeqNo\", \"Name\"]\n",
    "\n",
    "data = [(\"1\", \"john jones\"),\n",
    "        (\"2\", \"tracey smith\"),\n",
    "        (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.display()\n",
    "\n",
    "df2 = df.withColumn(\"NameUpperCase\", upperCase(col(\"Name\")))\n",
    "df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89193ee6-45fa-485e-a11e-cb992bb792e4",
     "showTitle": true,
     "title": "Create Schemas - Friendly way, easier to handle with delta tables etc."
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "\n",
    "def product_schema():\n",
    "    return T.StructType([\n",
    "        T.StructField(\"productID\", T.StringType(), True), \n",
    "        T.StructField(\"productName\", T.StringType(), True), \n",
    "        T.StructField(\"Description\", T.StringType(), True),\n",
    "        T.StructField(\"SKUID\", T.StringType(), True),\n",
    "        T.StructField(\"batchInputWeight\", T.FloatType(), True),\n",
    "        T.StructField(\"isProductLocked\", T.BooleanType(), True),\n",
    "        T.StructField(\"netWeight\", T.FloatType(), True),\n",
    "        T.StructField(\"GrossWeightFactor\", T.FloatType(), True),\n",
    "        T.StructField(\"servingSize\", T.FloatType(), True),\n",
    "        T.StructField(\"ltmSalesQuantity\", T.IntegerType(), True), \n",
    "        T.StructField(\"ltmPurchaseQuantity\", T.IntegerType(), True),\n",
    "        T.StructField(\"countryCode\", T.StringType(), True), \n",
    "        T.StructField(\"site\", T.StringType(), True), \n",
    "        T.StructField(\"certifiedImpacts\", T.StringType(), True), \n",
    "        T.StructField(\"certifiedStageImpacts\", T.StringType(), True), #\n",
    "        T.StructField(\"isCalculated\", T.StringType(), True),\n",
    "        T.StructField(\"ingredientCollections\", T.MapType(T.StringType(), T.StringType()), True), \n",
    "        T.StructField(\"salesActivities\", T.StringType(),True),\n",
    "        T.StructField(\"processingActivities\", T.StringType(), True),\n",
    "        T.StructField(\"packagingActivities\", T.StringType(), True),\n",
    "        T.StructField(\"storageActivities\", T.StringType(), True),\n",
    "        T.StructField(\"transportActivities\", T.StringType(), True),\n",
    "        T.StructField(\"commentary\", T.StringType(), True),\n",
    "        T.StructField(\"recipeCommentary\", T.StringType(), True),\n",
    "        T.StructField(\"saType\", T.StringType(), True),\n",
    "        T.StructField(\"taxonomyCode\", T.StringType(), True),\n",
    "        T.StructField(\"taxonomyL1Code\", T.StringType(), True),\n",
    "        T.StructField(\"taxonomyL2Code\", T.StringType(), True),\n",
    "        T.StructField(\"specificationVersion\", T.FloatType(), True),\n",
    "        T.StructField(\"categories\", T.StringType(), True) \n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68f74547-bbf8-4f8e-8d3d-74d2c79920a6",
     "showTitle": true,
     "title": "Anonymise Dataframe - Using pseudonyms technique"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sha2\n",
    "\n",
    "def pseudonymize_column(df, column_name, hash_length=64):\n",
    "    \"\"\"\n",
    "    Pseudonymize a column in a DataFrame using a hash function.\n",
    "\n",
    "    Parameters:\n",
    "        df (pyspark.sql.DataFrame): The input DataFrame.\n",
    "        column_name (str): The name of the column to be pseudonymized.\n",
    "        hash_length (int, optional): The length of the hashed output in characters. Default is 64.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: A new DataFrame with the pseudonymized column.\n",
    "    \"\"\"\n",
    "    # Using SHA-256 as the hash function, you can choose other hash functions if needed.\n",
    "    # Here, we use the sha2 function from PySpark to calculate the hash.\n",
    "    df_pseudonymized = df.withColumn(column_name + \"_pseudonymized\", sha2(df[column_name], hash_length))\n",
    "\n",
    "    # Drop the original column if desired.\n",
    "    df_pseudonymized = df_pseudonymized.drop(column_name)\n",
    "\n",
    "    # Optionally, you can rename the pseudonymized column to the original column name.\n",
    "    df_pseudonymized = df_pseudonymized.withColumnRenamed(column_name + \"_pseudonymized\", column_name)\n",
    "\n",
    "    return df_pseudonymized\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"Pseudonymization\").getOrCreate()\n",
    "\n",
    "    # Sample data for demonstration purposes.\n",
    "    data = [(\"John Doe\", 30), (\"Jane Smith\", 25), (\"Bob Johnson\", 35)]\n",
    "    columns = [\"name\", \"age\"]\n",
    "\n",
    "    df = spark.createDataFrame(data, columns)\n",
    "\n",
    "    # Pseudonymize the 'name' column using the pseudonymize_column function.\n",
    "    df_pseudonymized = pseudonymize_column(df, \"name\")\n",
    "\n",
    "    df_pseudonymized.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af8ebd81-d139-49c4-a4eb-c8a42644b773",
     "showTitle": true,
     "title": "Nice Cluster Settings"
    }
   },
   "source": [
    "Some of them are automatically enabled in the version 12.2 LTS of databricks, but it's good to be explicit\n",
    "\n",
    "- spark.sql.adaptive.enabled true\n",
    "- spark.databricks.delta.optimizeWrite.enabled true\n",
    "- spark.sql.adaptive.coalescePartitions.enabled true\n",
    "- spark.sql.adaptive.skewJoin.enabled true\n",
    "- spark.databricks.io.cache.enabled true\n",
    "- spark.databricks.delta.autoCompact.enabled true\n",
    "\n",
    "- spark.sql.shuffle.partitions auto  \n",
    "  (200 is the default --> If you data is small, reduce this number otherwise make it bigger - you need to play with numbers to fine tune. By assigning auto you let spark decides what may be good for general purpose workloads or commom ETLs)\n",
    "\n",
    "\n",
    "When using pandas workloads (toPandas etc) you can leverage the PyArrow serializtion for more efficiency and performance (make sure PyArrow is installed)\n",
    "\n",
    "- spark.sql.execution.arrow.pyspark.enabled true\n",
    "- spark.sql.execution.arrow.pyspark.fallback.enabled\n",
    "\n",
    "Hints:"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Swiss Knife",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
